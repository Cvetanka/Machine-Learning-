Predicted performances of high tech fitness geeks

This report is aimed to to evaluate some machine learning algorithms, by predicting the future performances of high tech geeks based on the past data collected from devices such as Jawbone Up, Nike FuelBand, and Fitbit. There are five posiblle outcomes and consequentually 5 posiblle predictions:

A: exactly according to the specification
B: throwing the elbows to the front
C: lifting the dumbbell only halfway
D: lowering the dumbbell only halfway
E: throwing the hips to the front
More information on this site: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

Getting and cleaning the data

The data are download from these sources: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv - (training data) https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv - (validation data)

The raw data contain a lot of columns that are either empty, NA, strange simbols or simply do not contribute to the analysis. Those columns are removed.

library(Hmisc)
library(caret)
library(randomForest)
library(foreach)
library(doParallel)
library(kernlab)
library(knitr)
library(doSNOW)
set.seed(12345)

train1 <- read.csv("pml-training.csv", na.strings=c("#DIV/0!","","NA"))
test1 <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!","","NA"))
train1 <- train1[colSums(is.na(train1))==0]
test1 <- test1[colSums(is.na(test1))==0]
setdiff(names(train1),names(test1))
##  [1] "user_name"            "raw_timestamp_part_1" "raw_timestamp_part_2"
##  [4] "cvtd_timestamp"       "new_window"           "num_window"          
##  [7] "roll_belt"            "pitch_belt"           "yaw_belt"            
## [10] "total_accel_belt"     "classe"
drops <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp", "new_window", "num_window","roll_belt","pitch_belt", "yaw_belt","total_accel_belt")
tr <- train1[,!(names(train1)%in% drops)]
ts <- test1[,!(names(test1)%in% drops)]
tr$classe <- as.factor(tr$classe)
Analysis - prediction and cross-validation

The training data set is split in ratio (0.75/0.25) on two sets: training and testing (a set used for cross-validation). Three different machine learning methods (Random Forest -RF, Support Vector Machine - SVM and K Nearest Neighbors - KNN) are applied and their accuracy is compared. They are applied two times in two different ways: 1) without preprocessing and 2) with preprocessing the data. An interesting observation is that the RF who has has the highest accuracy without preprocessing does not benefit in the term of accuracy from preprocessing, while SVM and KNN who have lower accuracy become more accurate with preprocessing.

Without preprocessing: Random_Forest (0.984) SVM (0.912) KNN(0.873)
With preprocessing: Random_Forest (0.984) SVM (0.972) KNN( 0.947)

Since preprocessing has small impact on the accuracy (the results are very nice without preprocessing), in the further analysis preprocessing is not used. The RF also differ from SVM and KNN in the selection of the variable of Importance as could be observed from the results bellow.

Random Forest is built on 500 trees, SVM has 7194 support vectors, while KNN uses 5-nearest neighbor classification model. 

Cross validation for all three methods is done on testing set. ConfusionMatrix and Statistics show good correlation between the outcome from training and testing sets (very small, almost neglecting number of misclassification).

Model1: Testing set: 49 out of 4904 or 1%

Model 2: Testing set: 369 out of 4904 or 7.5%

Model 3: Testing set: 423 out of 4904 or 8.6%

Kappa value in all cases is 1 (or very close to 1), what does mean that the observed accuracy perfectly matches the expected accuracy and the error is minimised. I would also like to mention that in parallel with the above methods boosted trees (“gbm”) and linear discriminant analysis (“lda”) model have been examined as well. The accuracy of gbm is 0.907, while lda has surprisingly low accuracy of 0.674. Due to the scope of this report, these analysis will not be presented.

All three models (rf, SVM, KNN) are further employed to build combined predictors and make final prediction based on both approaches: combined predictors and single model predictors as well. All four predictions (rf, SVM, KNN and combined) agree in all values and they are correct. Most important variables of the combined predictors model are presented.

set <- createDataPartition(y = tr$classe, p=0.75, list = FALSE)
training <- tr[set,]
testing <- tr[-set,]
registerDoParallel()

ConV <- trainControl(method = "cv", number = 3, allowParallel = TRUE, verboseIter = TRUE)
model1 <- train(classe ~., data = training, method = "rf", trControl =ConV) # without preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 2 on full training set
model2 <- train(classe~., data = training, method = "svmRadial", trControl = ConV)# without preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting sigma = 0.0149, C = 1 on full training set
model3 <- train(classe~., data = training, method = "knn", trControl = ConV) # without preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting k = 5 on full training set
Random_Forest <- round(max(head(model1$results)$Accuracy),3)
SVM <- round(max(head(model2$results)$Accuracy),3)
KNN <- round(max(head(model3$results)$Accuracy),3)
s <- data.frame(Random_Forest,SVM,KNN); rownames(s) <- "Accuracy"; s
##          Random_Forest   SVM   KNN
## Accuracy         0.984 0.912 0.872
model11 <- train(classe ~., data = training, method = "rf", trControl =ConV, preProc = c("center", "scale"),
tuneLength = 8)# with preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 8 on full training set
model22 <- train(classe~., data = training, method = "svmRadial", trControl = ConV, preProc = c("center", "scale"),
tuneLength = 8) # # with preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting sigma = 0.0145, C = 32 on full training set
model33 <- train(classe~., data = training, method = "knn", trControl = ConV,preProc = c("center", "scale"),
tuneLength = 8) # # with preprocessing
## Aggregating results
## Selecting tuning parameters
## Fitting k = 5 on full training set
Random_Forest <- round(max(head(model11$results)$Accuracy),3)
SVM <- round(max(head(model22$results)$Accuracy),3)
KNN <- round(max(head(model33$results)$Accuracy),3)
s <- data.frame(Random_Forest,SVM,KNN); rownames(s) <- "Accuracy"; s
##          Random_Forest   SVM   KNN
## Accuracy         0.985 0.971 0.946
(L <- list(model1$finalModel, model2$finalModel, model3$finalModel))
[[1]]

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 0.98%
Confusion matrix:
     A    B    C    D    E class.error
A 4178    3    2    1    1  0.00167264
B   24 2816    8    0    0  0.01123596
C    0   24 2541    2    0  0.01012855
D    0    0   62 2345    5  0.02777778
E    0    2    1    9 2694  0.00443459

[[2]]
Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
 parameter : cost C = 1 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  0.014912495421344 

Number of Support Vectors : 7194 

Objective Function Value : -1127.457 -839.0254 -740.4779 -459.3898 -1090.924 -572.4768 -785.2665 -1057.049 -759.5258 -644.7979 
Training error : 0.065838 

[[3]]
5-nearest neighbor classification model

Call:
knn3.matrix(x = as.matrix(x), y = y, k = param$k)

Training set class distribution:

   A    B    C    D    E 
4185 2848 2567 2412 2706 

R1 <- varImp(model1); R2 <- varImp(model2); R3 <- varImp(model3)
(R <- list(R1,R2,R3))
## [[1]]
## rf variable importance
## 
##   only 20 most important variables shown (out of 48)
## 
##                      Overall
## magnet_dumbbell_z     100.00
## magnet_dumbbell_y      88.70
## pitch_forearm          80.72
## roll_forearm           76.52
## magnet_dumbbell_x      74.19
## magnet_belt_z          71.61
## accel_belt_z           71.50
## magnet_belt_y          69.69
## accel_dumbbell_y       62.50
## roll_dumbbell          58.44
## accel_dumbbell_z       53.91
## roll_arm               53.71
## gyros_belt_z           50.62
## accel_forearm_x        47.49
## accel_dumbbell_x       42.03
## yaw_dumbbell           40.64
## gyros_dumbbell_y       40.21
## magnet_forearm_z       39.05
## magnet_arm_x           38.84
## total_accel_dumbbell   37.39
## 
## [[2]]
## ROC curve variable importance
## 
##   variables are sorted by maximum importance across the classes
##   only 20 most important variables shown (out of 48)
## 
##                        A     B     C      D     E
## pitch_forearm     100.00 62.91 74.64 100.00 66.57
## roll_dumbbell      50.90 62.68 83.08  83.08 58.27
## accel_forearm_x    81.53 53.94 64.36  81.53 48.16
## magnet_arm_x       77.71 52.53 54.29  77.71 65.78
## magnet_arm_y       76.27 39.80 52.59  76.27 68.91
## accel_arm_x        73.09 51.01 47.23  73.09 62.74
## pitch_dumbbell     55.70 72.34 72.34  63.51 47.66
## magnet_forearm_x   72.31 52.98 39.33  72.31 45.04
## magnet_belt_y      68.91 60.43 61.35  62.82 68.91
## magnet_dumbbell_x  66.77 66.77 65.27  51.84 53.05
## magnet_dumbbell_y  47.63 66.43 66.43  46.71 53.45
## magnet_dumbbell_z  59.32 27.16 59.32  35.53 55.84
## accel_dumbbell_x   59.20 59.20 58.49  51.01 41.90
## magnet_belt_z      53.37 50.57 49.97  53.14 53.37
## magnet_arm_z       51.96 51.96 36.77  39.77 50.78
## pitch_arm          51.17 27.46 38.38  42.30 51.17
## magnet_forearm_y   39.04 25.22 46.91  46.91 36.25
## accel_dumbbell_z   45.93 45.93 43.14  23.59 33.50
## yaw_dumbbell       23.27 45.12 45.12  22.71 30.85
## accel_belt_z       43.21 19.29 20.78  16.14 43.21
## 
## [[3]]
## ROC curve variable importance
## 
##   variables are sorted by maximum importance across the classes
##   only 20 most important variables shown (out of 48)
## 
##                        A     B     C      D     E
## pitch_forearm     100.00 62.91 74.64 100.00 66.57
## roll_dumbbell      50.90 62.68 83.08  83.08 58.27
## accel_forearm_x    81.53 53.94 64.36  81.53 48.16
## magnet_arm_x       77.71 52.53 54.29  77.71 65.78
## magnet_arm_y       76.27 39.80 52.59  76.27 68.91
## accel_arm_x        73.09 51.01 47.23  73.09 62.74
## pitch_dumbbell     55.70 72.34 72.34  63.51 47.66
## magnet_forearm_x   72.31 52.98 39.33  72.31 45.04
## magnet_belt_y      68.91 60.43 61.35  62.82 68.91
## magnet_dumbbell_x  66.77 66.77 65.27  51.84 53.05
## magnet_dumbbell_y  47.63 66.43 66.43  46.71 53.45
## magnet_dumbbell_z  59.32 27.16 59.32  35.53 55.84
## accel_dumbbell_x   59.20 59.20 58.49  51.01 41.90
## magnet_belt_z      53.37 50.57 49.97  53.14 53.37
## magnet_arm_z       51.96 51.96 36.77  39.77 50.78
## pitch_arm          51.17 27.46 38.38  42.30 51.17
## magnet_forearm_y   39.04 25.22 46.91  46.91 36.25
## accel_dumbbell_z   45.93 45.93 43.14  23.59 33.50
## yaw_dumbbell       23.27 45.12 45.12  22.71 30.85
## accel_belt_z       43.21 19.29 20.78  16.14 43.21
p <- predict(model1, newdata = testing)
confusionMatrix(p,testing$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1393   10    0    0    0
##          B    2  933    3    0    0
##          C    0    6  852   18    2
##          D    0    0    0  786    5
##          E    0    0    0    0  894
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9906          
##                  95% CI : (0.9875, 0.9931)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9881          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9986   0.9831   0.9965   0.9776   0.9922
## Specificity            0.9972   0.9987   0.9936   0.9988   1.0000
## Pos Pred Value         0.9929   0.9947   0.9704   0.9937   1.0000
## Neg Pred Value         0.9994   0.9960   0.9993   0.9956   0.9983
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2841   0.1903   0.1737   0.1603   0.1823
## Detection Prevalence   0.2861   0.1913   0.1790   0.1613   0.1823
## Balanced Accuracy      0.9979   0.9909   0.9950   0.9882   0.9961
pp <- predict(model2, newdata = testing)
confusionMatrix(pp,testing$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1390   90    2    5    2
##          B    2  819   32   12    9
##          C    3   39  813   77   40
##          D    0    0    8  708   34
##          E    0    1    0    2  816
## 
## Overall Statistics
##                                           
##                Accuracy : 0.927           
##                  95% CI : (0.9194, 0.9341)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9075          
##  Mcnemar's Test P-Value : < 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9964   0.8630   0.9509   0.8806   0.9057
## Specificity            0.9718   0.9861   0.9607   0.9898   0.9993
## Pos Pred Value         0.9335   0.9371   0.8364   0.9440   0.9963
## Neg Pred Value         0.9985   0.9677   0.9893   0.9769   0.9792
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2834   0.1670   0.1658   0.1444   0.1664
## Detection Prevalence   0.3036   0.1782   0.1982   0.1529   0.1670
## Balanced Accuracy      0.9841   0.9246   0.9558   0.9352   0.9525
ppp <- predict(model3, newdata = testing)
confusionMatrix(ppp,testing$classe)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1335   45   13   10    8
##          B   21  816   26    9   32
##          C   22   44  786   47   20
##          D   13   20   16  727   24
##          E    4   24   14   11  817
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9137          
##                  95% CI : (0.9055, 0.9215)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2e-16         
##                                           
##                   Kappa : 0.8909          
##  Mcnemar's Test P-Value : 3.5e-06         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9570   0.8599   0.9193   0.9042   0.9068
## Specificity            0.9783   0.9777   0.9672   0.9822   0.9868
## Pos Pred Value         0.9461   0.9027   0.8553   0.9087   0.9391
## Neg Pred Value         0.9828   0.9667   0.9827   0.9812   0.9792
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2722   0.1664   0.1603   0.1482   0.1666
## Detection Prevalence   0.2877   0.1843   0.1874   0.1631   0.1774
## Balanced Accuracy      0.9677   0.9188   0.9432   0.9432   0.9468
combined <- data.frame(p,pp,ppp,classe = testing$classe)
model_c <- train(classe ~., data=combined, method = "rf")
varImp(model_c)
## rf variable importance
## 
##      Overall
## pB   100.000
## pC    85.060
## pD    73.422
## pE    72.305
## ppE   42.843
## ppC   23.961
## ppB   19.710
## ppD   15.144
## pppE  12.454
## pppB   8.012
## pppD   4.390
## pppC   0.000

model_c$finalModel
Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 1%
Confusion matrix:
     A   B   C   D   E class.error
A 1390   2   3   0   0 0.003584229
B   10 933   6   0   0 0.016859852
C    0   3 852   0   0 0.003508772
D    0   0  18 786   0 0.022388060
E    0   0   2   5 894 0.007769145

Final validation

Final validation is done on validation set. All three methods give the same prediction. Answers as presented bellow are submitted and the are correct.

answers1 <- predict(model1, newdata=ts)
answers2 <- predict(model2, newdata=ts)
answers3 <- predict(model3, newdata=ts)

comb_pred <- data.frame(p = answers1, pp = answers2, ppp=answers3)
answers_c <- predict(model_c, comb_pred)

Prediction <- data.frame(answers1, answers2, answers3, answers_c)
colnames(Prediction) <- c("Random Forest", "SVM", "KNN", "Combined Predictors")
Prediction
##    Random Forest SVM KNN Combined Predictors
## 1              B   B   B                   B
## 2              A   A   A                   A
## 3              B   B   B                   B
## 4              A   A   A                   A
## 5              A   A   A                   A
## 6              E   E   E                   E
## 7              D   D   D                   D
## 8              B   B   B                   B
## 9              A   A   A                   A
## 10             A   A   A                   A
## 11             B   B   B                   B
## 12             C   C   C                   C
## 13             B   B   B                   B
## 14             A   A   A                   A
## 15             E   E   E                   E
## 16             E   E   E                   E
## 17             A   A   A                   A
## 18             B   B   B                   B
## 19             B   B   B                   B
## 20             B   B   B                   B
