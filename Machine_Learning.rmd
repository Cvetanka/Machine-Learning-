# Predicted performances of high tech fitness geeks #
========================================================

This report is aimed to to evaluate some machine learning algorithms, by predicting the future performances of high tech geeks based on the past data collected from devices such as Jawbone Up, Nike FuelBand, and Fitbit. There are five possible outcomes and consequentially 5 possible predictions:

    A: exactly according to the specification
    B: throwing the elbows to the front
    C: lifting the dumbbell only halfway
    D: lowering the dumbbell only halfway
    E: throwing the hips to the front

More information on this site: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Getting and cleaning the data ##

The data are download from these sources:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv - (training data)
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv - (validation data)

The raw data contain a lot of columns that are either empty, NA, strange symbols or simply do not contribute to the analysis. Those columns are removed. 

```{r, cache = TRUE }
library(Hmisc)
library(caret)
library(randomForest)
library(foreach)
library(doParallel)
library(kernlab)
library(knitr)
library(doSNOW)
set.seed(12345)

train1 <- read.csv("pml-training.csv", na.strings=c("#DIV/0!","","NA"))
test1 <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!","","NA"))
train1 <- train1[colSums(is.na(train1))==0]
test1 <- test1[colSums(is.na(test1))==0]
setdiff(names(train1),names(test1))
drops <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp", "new_window", "num_window","roll_belt","pitch_belt", "yaw_belt","total_accel_belt")
tr <- train1[,!(names(train1)%in% drops)]
ts <- test1[,!(names(test1)%in% drops)]
tr$classe <- as.factor(tr$classe)


```            

## Analysis - prediction and cross-validation ##

The training data set is split in ratio (0.75/0.25) on two sets: training and testing (a set used for cross-validation).
Three different machine learning methods (Random Forest -RF, Support Vector Machine - SVM and K Nearest Neighbors - KNN) are applied and their accuracy is compared. They are applied two times in two different ways: 1) without preprocessing and 2) with preprocessing the data. An interesting observation is that the RF who has has the highest accuracy without preprocessing does not benefit in the term of accuracy from preprocessing, while SVM and KNN who have lower accuracy become more accurate with preprocessing.
  
Without preprocessing: Random_Forest (0.984) SVM (0.912)    KNN(0.873)                                
With preprocessing:   Random_Forest (0.984) SVM (0.972)    KNN( 0.947)
           
Since preprocessing has small impact on the accuracy (the results are very nice without preprocessing), in the further analysis preprocessing is not used. 
The RF also differ from SVM and KNN in the selection of the variable of Importance as could be observed from the results bellow.

Cross validation for all three methods is done on testing set. ConfusionMatrix and Statistics show good correlation between the outcome from training and testing sets (very small, almost neglecting number of misclassification). 

Model1:  Testing set: 49 out of 4904 or 1%

Model 2: Testing set: 369 out of 4904 or 7.5%

Model 3: Testing set: 423 out of 4904 or 8.6%

Kappa value in all cases is 1 (or very close to 1), what does mean that the observed accuracy perfectly matches the expected accuracy and the error is minimised.
I would also like to mention that in parallel with the above methods boosted trees ("gbm") and linear discriminant analysis ("lda") model have been examined as well. The accuracy of gbm is 0.907, while lda has surprisingly low accuracy of 0.674. Due to the scope of this report, these analysis will not be presented. 

All three models (rf, SVM, KNN) are further employed to build combined predictors and make final prediction based on both approaches: combined predictors and single model predictors as well. All four predictions (rf, SVM, KNN and combined) agree in all values and they are correct.
Most important variables of the combined predictors model are presented.
```{r}

set <- createDataPartition(y = tr$classe, p=0.75, list = FALSE)
training <- tr[set,]
testing <- tr[-set,]
registerDoParallel()

ConV <- trainControl(method = "cv", number = 3, allowParallel = TRUE, verboseIter = TRUE)
model1 <- train(classe ~., data = training, method = "rf", trControl =ConV) # without preprocessing
model2 <- train(classe~., data = training, method = "svmRadial", trControl = ConV)# without preprocessing
model3 <- train(classe~., data = training, method = "knn", trControl = ConV) # without preprocessing

Random_Forest <- round(max(head(model1$results)$Accuracy),3)
SVM <- round(max(head(model2$results)$Accuracy),3)
KNN <- round(max(head(model3$results)$Accuracy),3)
s <- data.frame(Random_Forest,SVM,KNN); rownames(s) <- "Accuracy"; s

model11 <- train(classe ~., data = training, method = "rf", trControl =ConV, preProc = c("center", "scale"),
tuneLength = 8)# with preprocessing
model22 <- train(classe~., data = training, method = "svmRadial", trControl = ConV, preProc = c("center", "scale"),
tuneLength = 8) # # with preprocessing
model33 <- train(classe~., data = training, method = "knn", trControl = ConV,preProc = c("center", "scale"),
tuneLength = 8) # # with preprocessing



Random_Forest <- round(max(head(model11$results)$Accuracy),3)
SVM <- round(max(head(model22$results)$Accuracy),3)
KNN <- round(max(head(model33$results)$Accuracy),3)
s <- data.frame(Random_Forest,SVM,KNN); rownames(s) <- "Accuracy"; s

R1 <- varImp(model1); R2 <- varImp(model2); R3 <- varImp(model3)
(R <- list(R1,R2,R3))


p <- predict(model1, newdata = testing)
confusionMatrix(p,testing$classe)


pp <- predict(model2, newdata = testing)
confusionMatrix(pp,testing$classe)


ppp <- predict(model3, newdata = testing)
confusionMatrix(ppp,testing$classe)

combined <- data.frame(p,pp,ppp,classe = testing$classe)
model_c <- train(classe ~., data=combined, method = "rf")
varImp(model_c)

```

## Final validation ##

Final validation is done on validation set. All three methods give the same prediction. Answers as presented bellow are submitted and the are correct.

```{r }
 
answers1 <- predict(model1, newdata=ts)
answers2 <- predict(model2, newdata=ts)
answers3 <- predict(model3, newdata=ts)
comb_pred <- data.frame(p = answers1, pp = answers2, ppp=answers3)

Prediction <- data.frame(answers1, answers2, answers3, comb_pred)
colnames(Prediction) <- c("Random Forest", "SVM", "KNN", "Combined Predictors")
Prediction

```

